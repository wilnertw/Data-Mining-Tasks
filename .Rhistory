summary(abalone)
set.seed(123)
data <- abalone[, c("Length","Diameter", "Whole.weight","Height","Rings")]
data <- abalone[, c("Length","Diameter", "Whole Weight","Height","Rings")]
View(abalone)
data <- abalone[, c("Length","Diameter", "Whole.weight","Height","Rings")]
abalone <- read.csv("abalone.csv")
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone <- as.data.frame(lapply(abalone, function(x) if(is.numeric(x)) normalize(x) else x))
# We won't use 'Rings' and 'Age', so let's exclude them
abalone <- abalone[, !names(abalone) %in% c("Rings", "Age")]
# Convert the 'Sex' column to a factor if it's not already
abalone$Sex <- as.factor(abalone$Sex)
# Convert 'Whole weight' to numeric, ensuring that it's in the correct format
# First, check if it's not already numeric to avoid unnecessary conversion
if(!is.numeric(abalone$`Whole weight`)) {
abalone$`Whole weight` <- as.numeric(as.character(abalone$`Whole weight`))
}
abalone$Whole.weight <- as.numeric(as.character(abalone$Whole.weight))
# Convert 'Whole weight' to numeric, ensuring that it's in the correct format
# First, check if it's not already numeric to avoid unnecessary conversion
if(!is.numeric(abalone$Whole.weight)) {
abalone$Whole.weight <- as.numeric(as.character(abalone$Whole.weight))
}
# Now, proceed with creating the 'Class' attribute using 'cut'
abalone$Class <- cut(abalone$`Whole weight`,
breaks = quantile(abalone$`Whole weight`, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
labels = c("Young", "Adult", "Old"),
include.lowest = TRUE)
# Now, proceed with creating the 'Class' attribute using 'cut'
abalone$Class <- cut(abalone$Whole.weight,
breaks = quantile(abalone$Whole.weight, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
labels = c("Young", "Adult", "Old"),
include.lowest = TRUE)
# Ensure 'Class' is a factor
abalone$Class <- as.factor(abalone$Class)
# Splitting the dataset into training and testing sets
set.seed(123)  # For reproducibility
indexes <- sample(1:nrow(abalone), size = 0.75 * nrow(abalone))
train_data <- abalone[indexes, ]
test_data <- abalone[-indexes, ]
# KNN model training and prediction
# Using kknn from the kknn package
fit <- kknn(Class ~ ., train_data, test_data, k = 5, distance = 1, kernel = "optimal")
# Predictions
predictions <- fit$fitted.values
# Convert predictions to factors for classification accuracy evaluation
predicted_classes <- factor(predictions, levels = levels(test_data$Class))
# Evaluating model performance
confusionMatrix <- table(Predicted = predicted_classes, Actual = test_data$Class)
print(confusionMatrix)
# Calculate accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
print(paste("Accuracy:", accuracy))
# Load required libraries for plotting
library(ggplot2)
# Add the predicted classes to the test_data for plotting
test_data$PredictedClass <- predicted_classes
# Scatter plot using ggplot2
ggplot(test_data, aes(x = Length, y = Diameter, color = PredictedClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = "Abalone Age Class Predictions",
x = "Length (normalized)",
y = "Diameter (normalized)",
color = "Class") +
theme_minimal()
# Using kknn from the kknn package
knn_3 <- kknn(Class ~ ., train_data, test_data, k = 3, distance = 1, kernel = "optimal")
# Predictions
predictions <- knn_3$fitted.values
# Convert predictions to factors for classification accuracy evaluation
predicted_classes <- factor(predictions, levels = levels(test_data$Class))
# Evaluating model performance
confusionMatrix <- table(Predicted = predicted_classes, Actual = test_data$Class)
print(confusionMatrix)
# Calculate accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
print(paste("Accuracy:", accuracy))
# Load required libraries for plotting
library(ggplot2)
# Add the predicted classes to the test_data for plotting
test_data$PredictedClass <- predicted_classes
# Scatter plot using ggplot2
ggplot(test_data, aes(x = Length, y = Diameter, color = PredictedClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = "Abalone Age Class Predictions",
x = "Length (normalized)",
y = "Diameter (normalized)",
color = "Class") +
theme_minimal()
abalone$ActualClass <- cut(abalone$Rings,
breaks = c(-Inf, 2, 5, Inf),
labels = c("Young", "Adult", "Old"),
right = TRUE)
# Load the necessary library
library(kknn)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- as.data.frame(lapply(abalone, function(x) if(is.numeric(x)) normalize(x) else x))
#Exclude Rings and Age attributes
abalone_normalized <- abalone_normalized[, !names(abalone) %in% c("Rings", "Age")]
#convert Sex attributes to factors
abalone_normalized$Sex <- as.factor(abalone_normalized$Sex)
# Convert Whole.weight to numeric
# First, check if it's not already numeric to avoid unnecessary conversion
if(!is.numeric(abalone_normalized$Whole.weight)) {
abalone_normalized$Whole.weight <- as.numeric(as.character(abalone_normalized$Whole.weight))
}
# Now, proceed with creating the 'Class' attribute using 'cut'
abalone_normalized$Class <- cut(abalone_normalized$Whole.weight,
breaks = quantile(abalone_normalized$Whole.weight, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
labels = c("Young", "Adult", "Old"),
include.lowest = TRUE)
# set Class as factor
abalone_normalized$Class <- as.factor(abalone$Class)
View(abalone)
abalone_normalized$Class)
# set Class as factor
abalone_normalized$Class <- as.factor(abalone_normalized$Class)
# Splitting the dataset into training and testing sets
set.seed(123)
indexes <- sample(1:nrow(abalone_normalized), size = 0.75 * nrow(abalone_normalized))
train_data <- abalone_normalized[indexes, ]
test_data <- abalone_normalized[-indexes, ]
# Using kknn from the kknn package
knn_3 <- kknn(Class ~ ., train_data, test_data, k = 3, distance = 1, kernel = "optimal")
# Predictions
predictions <- knn_3$fitted.values
# Convert predictions to factors for classification accuracy evaluation
predicted_classes <- factor(predictions, levels = levels(test_data$Class))
# Evaluating model performance
confusionMatrix <- table(Predicted = predicted_classes, Actual = test_data$Class)
print(confusionMatrix)
# Accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
print(paste("Accuracy:", accuracy))
# Load required libraries for plotting
library(ggplot2)
# Add the predicted classes to the test_data for plotting
test_data$PredictedClass <- predicted_classes
# Scatter plot using ggplot2
ggplot(test_data, aes(x = Length, y = Diameter, color = PredictedClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = "Abalone Age Class Predictions",
x = "Length (normalized)",
y = "Diameter (normalized)",
color = "Class") +
theme_minimal()
abalone$ActualClass <- cut(abalone$Rings,
breaks = c(-Inf, 2, 5, Inf),
labels = c("Young", "Adult", "Old"),
right = TRUE)
# Assuming test_data already has 'ActualClass' based on 'Rings'
test_data$ActualClass <- factor(ifelse(test_data$Rings <= 8, "Young",
ifelse(test_data$Rings <= 10, "Adult", "Old")))
View(abalone_normalized)
# Assuming test_data already has 'ActualClass' based on 'Rings'
test_data$ActualClass <- factor(ifelse(test_data$Rings <= 2, "Young",
ifelse(test_data$Rings <= 5, "Adult", "Old")))
# Reading the dataset
abalone <- read.csv("abalone.csv")
View(test_data)
View(abalone)
abalone$ActualClass <- cut(abalone$Rings,
breaks = c(-Inf, 2, 5, Inf),
labels = c("Young", "Adult", "Old"),
right = TRUE)
View(abalone)
View(test_data)
View(train_data)
View(test_data)
View(test_data)
# Assuming test_data already has 'ActualClass' based on 'Rings'
test_data$PredictedClass <- factor(ifelse(test_data$Rings <= 2, "Young",
ifelse(test_data$Rings <= 5, "Adult", "Old")))
ggplot(abalone, aes(x = Length, y = Diameter, color = ActualClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = " Actual Abalone Age Class",
x = "Length",
y = "Diameter",
color = "Class") +
theme_minimal()
abalone$ActualClass <- cut(abalone$Rings,
breaks = c(-Inf, 3, 5, Inf),
labels = c("Young", "Adult", "Old"),
right = TRUE)
ggplot(abalone, aes(x = Length, y = Diameter, color = ActualClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = " Actual Abalone Age Class",
x = "Length",
y = "Diameter",
color = "Class") +
theme_minimal()
abalone$ActualClass <- cut(abalone$Rings,
breaks = c(-Inf, 5, 10, Inf),
labels = c("Young", "Adult", "Old"),
right = TRUE)
ggplot(abalone, aes(x = Length, y = Diameter, color = ActualClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = " Actual Abalone Age Class",
x = "Length",
y = "Diameter",
color = "Class") +
theme_minimal()
View(abalone_normalized)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- as.data.frame(lapply(abalone, function(x) if(is.numeric(x)) normalize(x) else x))
#Exclude Rings and Age attributes
data <- abalone_normalized[, !names(abalone) %in% c("Rings", "Age")]
#convert Sex attributes to factors
data$Sex <- as.factor(data$Sex)
# Convert Whole.weight to numeric
# First, check if it's not already numeric to avoid unnecessary conversion
if(!is.numeric(data$Whole.weight)) {
data$Whole.weight <- as.numeric(as.character(data$Whole.weight))
}
# Now, proceed with creating the 'Class' attribute using 'cut'
data$Class <- cut(data$Whole.weight,
breaks = quantile(data$Whole.weight, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
labels = c("Young", "Adult", "Old"),
include.lowest = TRUE)
View(abalone_normalized)
View(abalone_normalized)
View(abalone)
# Scatter plot using ggplot2
ggplot(test_data, aes(x = Length, y = Diameter, color = PredictedClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = "Abalone Age Class Predictions",
x = "Length (normalized)",
y = "Diameter (normalized)",
color = "Class") +
theme_minimal()
ggplot(abalone, aes(x = Length, y = Diameter, color = ActualClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = " Actual Abalone Age Class",
x = "Length",
y = "Diameter",
color = "Class") +
theme_minimal()
abalone$ActualClass <- cut(abalone$Rings,
breaks = c(-Inf, 5, 10, Inf),
labels = c("Young", "Adult", "Old"),
right = TRUE)
ggplot(abalone, aes(x = Length, y = Diameter, color = ActualClass)) +
geom_point(alpha = 0.6) +
scale_color_manual(values = c("Young" = "blue", "Adult" = "green", "Old" = "red")) +
labs(title = " Actual Abalone Age Class",
x = "Length",
y = "Diameter",
color = "Class") +
theme_minimal()
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
#Exclude Rings and Age attributes
data <- abalone_normalized[, !names(abalone) %in% c("Rings", "Age")]
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
#Exclude Rings and Age attributes
data <- abalone_normalized[, !names(abalone) %in% c("Rings", "Age")]
# Load the necessary library
library(kknn)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
View(data)
View(abalone)
# Load the necessary library
library(kknn)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
data <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
#convert Sex attributes to factors
data$Sex <- as.factor(data$Sex)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
data <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
View(abalone)
data <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- abalone %>%
mutate(across(where(is.numeric) & !names(.) %in% c("Rings", "Age"), ~normalize(.)))
# Load the necessary library
library(kknn)
library(dplyr)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- abalone %>%
mutate(across(where(is.numeric) & !names(.) %in% c("Rings", "Age"), ~normalize(.)))
abalone_normalized <- abalone %>%
select(-c(Rings, Age)) %>%
mutate(across(where(is.numeric), ~normalize(.)))
abalone_normalized <- abalone %>%
select(-c(Rings, Age)) %>%
mutate(across(where(is.numeric), ~normalize(.)))
#convert Sex attributes to factors
data$Sex <- as.factor(data$Sex)
#convert Sex attributes to factors
data$Sex <- as.factor(data$Sex)
# Convert Whole.weight to numeric
# First, check if it's not already numeric to avoid unnecessary conversion
if(!is.numeric(data$Whole.weight)) {
data$Whole.weight <- as.numeric(as.character(data$Whole.weight))
}
# Now, proceed with creating the 'Class' attribute using 'cut'
data$Class <- cut(data$Whole.weight,
breaks = quantile(data$Whole.weight, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
labels = c("Young", "Adult", "Old"),
include.lowest = TRUE)
# set Class as factor
data$Class <- as.factor(data$Class)
# Splitting the dataset into training and testing sets
set.seed(123)
indexes <- sample(1:nrow(data), size = 0.75 * nrow(data))
train_data <- data[indexes, ]
test_data <- data[-indexes, ]
# Using kknn from the kknn package
knn_3 <- kknn(Class ~ ., train_data, test_data, k = 3, distance = 1, kernel = "optimal")
abalone_normalized <- abalone %>%
select(-c(Rings, Age)) %>%
mutate(across(where(is.numeric), ~normalize(.)))
#convert Sex attributes to factors
data$Sex <- as.factor(data$Sex)
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone_normalized <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
data <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
data <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
library(dplyr)
# Reading the dataset
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
data <- as.data.frame(lapply(abalone, function(x) {
if(is.numeric(x) && !(colnames(abalone) %in% c("Rings", "Age"))) {
normalize(x)
} else {
x
}
}))
#convert Sex attributes to factors
data$Sex <- as.factor(data$Sex)
# Convert Whole.weight to numeric
# First, check if it's not already numeric to avoid unnecessary conversion
if(!is.numeric(data$Whole.weight)) {
data$Whole.weight <- as.numeric(as.character(data$Whole.weight))
}
# Now, proceed with creating the 'Class' attribute using 'cut'
data$Class <- cut(data$Whole.weight,
breaks = quantile(data$Whole.weight, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),
labels = c("Young", "Adult", "Old"),
include.lowest = TRUE)
# set Class as factor
data$Class <- as.factor(data$Class)
# Splitting the dataset into training and testing sets
set.seed(123)
library(ggplot2)
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone <- as.data.frame(lapply(abalone, function(x) if(is.numeric(x)) normalize(x) else x))
summary(abalone)
set.seed(123)
data <- abalone[, c("Length","Diameter", "Whole.weight","Height")]
str(data)
k_3 <- kmeans(data, 3)
clus <- cbind(data, clus2 = k_3$cluster)
# Assuming clus is your data and k_3 is the result of k-means clustering
plot(clus$Diameter, clus$Rings, col = k_3$cluster, pch = k_3$cluster,
main = "Diameter vs Rings", xlim = c(0, 1), ylim = c(0, 1),
xlab = "Rings", ylab = "Diameter(mm)")
# Evaluating model performance
confusionMatrix <- table(Predicted = predicted_classes, Actual = test_data$Class)
print(confusionMatrix)
View(data)
library(ggplot2)
abalone <- read.csv("abalone.csv")
normalize <- function(x) {
return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
}
abalone <- as.data.frame(lapply(abalone, function(x) if(is.numeric(x)) normalize(x) else x))
summary(abalone)
set.seed(123)
data <- abalone[, c("Length","Diameter", "Whole.weight","Height")]
str(data)
k_3 <- kmeans(data, 3)
clus <- cbind(data, clus2 = k_3$cluster)
# Assuming clus is your data and k_3 is the result of k-means clustering
plot(clus$Diameter, clus$Rings, col = k_3$cluster, pch = k_3$cluster,
main = "Diameter vs Rings", xlim = c(0, 1), ylim = c(0, 1),
xlab = "Rings", ylab = "Diameter(mm)")
# Assuming clus is your data and k_3 is the result of k-means clustering
plot(clus$Diameter, clus$Rings, col = k_3$cluster, pch = k_3$cluster,
main = "Diameter vs Rings", xlim = c(0, 1), ylim = c(0, 1),
xlab = "Rings", ylab = "Diameter(mm)")
View(abalone)
clus <- cbind(data, clus2 = k_3$cluster)
# Assuming clus is your data and k_3 is the result of k-means clustering
plot(clus$Diameter, abalone$Rings, col = k_3$cluster, pch = k_3$cluster,
main = "Diameter vs Rings", xlim = c(0, 1), ylim = c(0, 1),
xlab = "Rings", ylab = "Diameter(mm)")
